{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f9c521-f794-4da6-a44f-ed03ee62be2f",
   "metadata": {},
   "source": [
    "# Run SICILIAN demonstration\n",
    "\n",
    "SICILIAN needs input data and the STAR aligner index for that reference genome. I have a test for STAR aligner with a single read that works on MyBinder, and so run that and then index will be here because that run makes the index as the third step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23e670-2be6-4a36-90c4-8bdfa4a0b68d",
   "metadata": {},
   "source": [
    "### Preparation STEP #1: Get input data and make STAR aligner index as part of aligning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187ffa3-3481-4ae4-8ac1-1506e280f3e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the STAR Aligner to prepare and make a tiny dataset and then index that mock reference genome with STAR aligner and then map a read to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7d808-1a4c-441a-b164-6fa16fc2c7b9",
   "metadata": {},
   "source": [
    "To do all that, we can run the Jupyter notebook file `STAR_aligner_run_on_MyBinder.ipynb` with the following command, I have added printing the steps to stderr (the pink highlighted lines) to better enable understanding the context of the steps without needing to actually open the notebook `STAR_aligner_run_on_MyBinder.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928fb930-4a8f-430f-9f3b-edbea2ad63f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Install STAR aligner to prepare\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      " - bioconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.3.0\n",
      "    latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /srv/conda/envs/notebook\n",
      "\n",
      "  added / updated specs:\n",
      "    - bioconda::star\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    htslib-1.20                |       h5efdd21_2         2.9 MB  bioconda\n",
      "    star-2.7.11b               |       h43eeafb_2         8.1 MB  bioconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  htslib             bioconda/linux-64::htslib-1.20-h5efdd21_2 \n",
      "  star               bioconda/linux-64::star-2.7.11b-h43eeafb_2 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "star-2.7.11b         | 8.1 MB    |                                       |   0% \n",
      "htslib-1.20          | 2.9 MB    |                                       |   0% \u001b[A\n",
      "htslib-1.20          | 2.9 MB    | 2                                     |   1% \u001b[A\n",
      "htslib-1.20          | 2.9 MB    | ###############8                      |  43% \u001b[A\n",
      "htslib-1.20          | 2.9 MB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run a test alignment to demonstrate it works\n",
      "1. Create a small FASTA file with a short sequence:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">test_seq\n",
      "ACGTGGACGTACCCGTACGTACCCAAGTACGAGTACGTACGGGTACGTTCCACAAGTACGT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2. Run `mkdir test_index` to make the directory to store the index."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x 2 jovyan jovyan 4.0K Jul 25 20:26 \u001b[0m\u001b[01;34mtest_index\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3. Generate a genome index using this small FASTA file:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t/srv/conda/envs/notebook/bin/STAR-avx2 --runMode genomeGenerate --genomeFastaFiles test.fa --genomeDir test_index --genomeSAindexNbases 1\n",
      "\tSTAR version: 2.7.11b   compiled: 2024-07-03T14:39:20+0000 :/opt/conda/conda-bld/star_1720017372352/work/source\n",
      "Jul 25 20:26:46 ..... started STAR run\n",
      "Jul 25 20:26:46 ... starting to generate Genome files\n",
      "Jul 25 20:26:46 ... starting to sort Suffix Array. This may take a long time...\n",
      "Jul 25 20:26:46 ... sorting Suffix Array chunks and saving them to disk...\n",
      "Jul 25 20:26:46 ... loading chunks from disk, packing SA...\n",
      "Jul 25 20:26:46 ... finished generating suffix array\n",
      "Jul 25 20:26:46 ... generating Suffix Array index\n",
      "Jul 25 20:26:46 ... completed Suffix Array index\n",
      "Jul 25 20:26:46 ... writing Genome to disk ...\n",
      "Jul 25 20:26:46 ... writing Suffix Array to disk ...\n",
      "Jul 25 20:26:46 ... writing SAindex to disk\n",
      "Jul 25 20:26:46 ..... finished successfully\n",
      "\t/srv/conda/envs/notebook/bin/STAR-avx2 --runMode alignReads --genomeDir test_index --readFilesIn test.fq --outFileNamePrefix test_output\n",
      "\tSTAR version: 2.7.11b   compiled: 2024-07-03T14:39:20+0000 :/opt/conda/conda-bld/star_1720017372352/work/source\n",
      "Jul 25 20:26:46 ..... started STAR run\n",
      "Jul 25 20:26:46 ..... loading genome\n",
      "Jul 25 20:26:46 ..... started mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Create a small FASTQ file with a few reads from the test sequence:\n",
      "5. Run the alignment using the small test files:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jul 25 20:26:46 ..... finished mapping\n",
      "Jul 25 20:26:46 ..... finished successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHOW THE FILES GENERATED BY ALL THAT AND THE ALIGNING STEP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 404K\n",
      "drwxr-x--- 1 jovyan jovyan 4.0K Jul 25 20:26 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 1 root   root   4.0K Jul 23 19:07 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan jovyan  220 Jan  6  2022 .bash_logout\n",
      "-rw-r--r-- 1 jovyan jovyan 3.7K Jan  6  2022 .bashrc\n",
      "drwxr-xr-x 3 jovyan jovyan 4.0K Jul 23 18:59 \u001b[01;34mbenchmarking\u001b[0m/\n",
      "drwxr-xr-x 1 jovyan jovyan 4.0K Jul 23 18:59 \u001b[01;34mbinder\u001b[0m/\n",
      "drwxr-xr-x 1 jovyan jovyan 4.0K Jul 23 19:03 \u001b[01;34m.cache\u001b[0m/\n",
      "drwxrwsr-x 1 jovyan jovyan 4.0K Jul 23 19:02 \u001b[01;34m.conda\u001b[0m/\n",
      "drwx------ 3 jovyan jovyan 4.0K Jul 25 20:19 \u001b[01;34m.config\u001b[0m/\n",
      "drwxr-xr-x 8 jovyan jovyan 4.0K Jul 23 18:59 \u001b[01;34m.git\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan jovyan   94 Jul 23 18:59 .gitignore\n",
      "drwxr-xr-x 2 jovyan jovyan 4.0K Jul 25 20:24 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
      "drwxr-xr-x 1 jovyan jovyan 4.0K Jul 25 20:24 \u001b[01;34m.ipython\u001b[0m/\n",
      "drwxr-xr-x 3 jovyan jovyan 4.0K Jul 25 20:21 \u001b[01;34m.jupyter\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan jovyan 8.3K Jul 25 20:26 .jupyter-server-log.txt\n",
      "-rw-r--r-- 1 jovyan jovyan  18K Jul 23 18:59 LICENSE\n",
      "drwxr-xr-x 1 jovyan jovyan 4.0K Jul 23 19:08 \u001b[01;34m.local\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan jovyan  807 Jan  6  2022 .profile\n",
      "-rw-r--r-- 1 jovyan jovyan  26K Jul 23 18:59 README.md\n",
      "-rw-r--r-- 1 jovyan jovyan   98 Jul 23 18:59 requirements.txt\n",
      "-rw-r--r-- 1 jovyan jovyan  570 Jul 25 20:24 run_class_input.sh\n",
      "-rw-r--r-- 1 jovyan jovyan  495 Jul 25 20:24 run_GLM.sh\n",
      "-rw-r--r-- 1 jovyan jovyan  889 Jul 25 20:24 run_map.sh\n",
      "-rw-r--r-- 1 jovyan jovyan  14K Jul 25 20:26 Run_SICILIAN_demo_on_MyBinder.ipynb\n",
      "-rw-r--r-- 1 jovyan jovyan 1.5K Jul 23 18:59 RUN_SICILIAN_post_process.sh\n",
      "drwxr-xr-x 1 jovyan jovyan 4.0K Jul 25 20:23 \u001b[01;34mscripts\u001b[0m/\n",
      "drwxr-xr-x 3 jovyan jovyan 4.0K Jul 25 20:24 \u001b[01;34msicilian_output\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan jovyan 141K Jul 23 18:59 SICILIAN.png\n",
      "-rw-r--r-- 1 jovyan jovyan 6.7K Jul 23 18:59 SICILIAN_post_process.py\n",
      "-rw-r--r-- 1 jovyan jovyan 9.2K Jul 25 20:24 SICILIAN.py\n",
      "-rw-r--r-- 1 jovyan jovyan  20K Jul 23 18:59 STAR_aligner_run_on_MyBinder.ipynb\n",
      "-rw-r--r-- 1 jovyan jovyan   72 Jul 25 20:26 test.fa\n",
      "-rw-r--r-- 1 jovyan jovyan   49 Jul 25 20:26 test.fq\n",
      "drwxr-xr-x 2 jovyan jovyan 4.0K Jul 25 20:26 \u001b[01;34mtest_index\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan jovyan  481 Jul 25 20:26 test_outputAligned.out.sam\n",
      "-rw-r--r-- 1 jovyan jovyan 2.0K Jul 25 20:26 test_outputLog.final.out\n",
      "-rw-r--r-- 1 jovyan jovyan 3.7K Jul 25 20:26 test_outputLog.out\n",
      "-rw-r--r-- 1 jovyan jovyan  246 Jul 25 20:26 test_outputLog.progress.out\n",
      "-rw-r--r-- 1 jovyan jovyan    0 Jul 25 20:26 test_outputSJ.out.tab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Examine results to see it worked\n",
      "Results in 'test_outputLog.final.out'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Started job on |\tJul 25 20:26:46\n",
      "                             Started mapping on |\tJul 25 20:26:46\n",
      "                                    Finished on |\tJul 25 20:26:46\n",
      "       Mapping speed, Million of reads per hour |\tinf\n",
      "\n",
      "                          Number of input reads |\t1\n",
      "                      Average input read length |\t19\n",
      "                                    UNIQUE READS:\n",
      "                   Uniquely mapped reads number |\t1\n",
      "                        Uniquely mapped reads % |\t100.00%\n",
      "                          Average mapped length |\t19.00\n",
      "                       Number of splices: Total |\t0\n",
      "            Number of splices: Annotated (sjdb) |\t0\n",
      "                       Number of splices: GT/AG |\t0\n",
      "                       Number of splices: GC/AG |\t0\n",
      "                       Number of splices: AT/AC |\t0\n",
      "               Number of splices: Non-canonical |\t0\n",
      "                      Mismatch rate per base, % |\t0.00%\n",
      "                         Deletion rate per base |\t0.00%\n",
      "                        Deletion average length |\t0.00\n",
      "                        Insertion rate per base |\t0.00%\n",
      "                       Insertion average length |\t0.00\n",
      "                             MULTI-MAPPING READS:\n",
      "        Number of reads mapped to multiple loci |\t0\n",
      "             % of reads mapped to multiple loci |\t0.00%\n",
      "        Number of reads mapped to too many loci |\t0\n",
      "             % of reads mapped to too many loci |\t0.00%\n",
      "                                  UNMAPPED READS:\n",
      "  Number of reads unmapped: too many mismatches |\t0\n",
      "       % of reads unmapped: too many mismatches |\t0.00%\n",
      "            Number of reads unmapped: too short |\t0\n",
      "                 % of reads unmapped: too short |\t0.00%\n",
      "                Number of reads unmapped: other |\t0\n",
      "                     % of reads unmapped: other |\t0.00%\n",
      "                                  CHIMERIC READS:\n",
      "                       Number of chimeric reads |\t0\n",
      "                            % of chimeric reads |\t0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Results in 'test_outputAligned.out.sam'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@HD\tVN:1.4\n",
      "@SQ\tSN:test_seq\tLN:61\n",
      "@PG\tID:STAR\tPN:STAR\tVN:2.7.11b\tCL:/srv/conda/envs/notebook/bin/STAR-avx2   --runMode alignReads      --genomeDir test_index   --readFilesIn test.fq      --outFileNamePrefix test_output\n",
      "@CO\tuser command line: /srv/conda/envs/notebook/bin/STAR-avx2 --runMode alignReads --genomeDir test_index --readFilesIn test.fq --outFileNamePrefix test_output\n",
      "read1\t0\ttest_seq\t19\t255\t19M\t*\t0\t0\tGTACCCAAGTACGAGTACG\tAAAAAAAAAAAAAAAAAAA\tNH:i:1\tHI:i:1\tAS:i:18\tnM:i:0\n"
     ]
    }
   ],
   "source": [
    "%run STAR_aligner_run_on_MyBinder.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5bcee-fbe3-4596-83a0-cead799ba1b2",
   "metadata": {},
   "source": [
    "### Preparation STEP #2: Use SICILIAN's `create_annotator.py` script to prepare for SICILIAN's main script to run\n",
    "\n",
    "The annotator pickle file for a GTF file needs to be made only once by the `create_annotator.py` script in the SICILIAN scripts directory.  \n",
    "Most established reference genomes would have associated GTF files you could obtain. The README for SICILAIN describes resources.\n",
    "\n",
    "For this demo, we'll first make a mock GTF file for this tiny test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c4ec57-a418-4d20-8b1e-71df9da61aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 's' (str) to file 'test_gtf_file.gtf'.\n"
     ]
    }
   ],
   "source": [
    "s='''test_seq\\tSTAR\\tgene\\t1\\t60\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\ttranscript\\t1\\t20\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_1\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\texon\\t1\\t20\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_1\"; exon_number \"1\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\ttranscript\\t21\\t40\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_2\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\texon\\t21\\t40\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_2\"; exon_number \"1\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\ttranscript\\t41\\t60\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_3\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\texon\\t41\\t60\\t.\\t+\\t.\\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_3\"; exon_number \"1\"; gene_name \"test_gene_1\";\n",
    "test_seq\\tSTAR\\tgene\\t1\\t60\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; gene_name \"test_gene_2\";\n",
    "test_seq\\tSTAR\\ttranscript\\t1\\t30\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_4\"; gene_name \"test_gene_2\";\n",
    "test_seq\\tSTAR\\texon\\t1\\t10\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_4\"; exon_number \"1\"; gene_name \"test_gene_2\";\n",
    "test_seq\\tSTAR\\texon\\t21\\t30\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_4\"; exon_number \"2\"; gene_name \"test_gene_2\";\n",
    "test_seq\\tSTAR\\ttranscript\\t31\\t60\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_5\"; gene_name \"test_gene_2\";\n",
    "test_seq\\tSTAR\\texon\\t31\\t40\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_5\"; exon_number \"1\"; gene_name \"test_gene_2\";\n",
    "test_seq\\tSTAR\\texon\\t51\\t60\\t.\\t+\\t.\\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_5\"; exon_number \"2\"; gene_name \"test_gene_2\";'''\n",
    "%store s >test_gtf_file.gtf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156416f-d6df-4f96-8d90-4e7875e1c842",
   "metadata": {},
   "source": [
    "I know that text above looks odd; however, it results in a nice tab-delmited file, which is what is expected by software using GTF files, see about the GTF/GFF file format [here](https://useast.ensembl.org/info/website/upload/gff.html).  \n",
    "Run the next cell to see that the result does look more as expeced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99434819-f204-4ea0-9bd4-fb5a57cf0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_seq\tSTAR\tgene\t1\t60\t.\t+\t.\tgene_id \"TEST_GENE_1\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\ttranscript\t1\t20\t.\t+\t.\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_1\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\texon\t1\t20\t.\t+\t.\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_1\"; exon_number \"1\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\ttranscript\t21\t40\t.\t+\t.\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_2\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\texon\t21\t40\t.\t+\t.\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_2\"; exon_number \"1\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\ttranscript\t41\t60\t.\t+\t.\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_3\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\texon\t41\t60\t.\t+\t.\tgene_id \"TEST_GENE_1\"; transcript_id \"TRANSCRIPT_3\"; exon_number \"1\"; gene_name \"test_gene_1\";\n",
      "test_seq\tSTAR\tgene\t1\t60\t.\t+\t.\tgene_id \"TEST_GENE_2\"; gene_name \"test_gene_2\";\n",
      "test_seq\tSTAR\ttranscript\t1\t30\t.\t+\t.\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_4\"; gene_name \"test_gene_2\";\n",
      "test_seq\tSTAR\texon\t1\t10\t.\t+\t.\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_4\"; exon_number \"1\"; gene_name \"test_gene_2\";\n",
      "test_seq\tSTAR\texon\t21\t30\t.\t+\t.\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_4\"; exon_number \"2\"; gene_name \"test_gene_2\";\n",
      "test_seq\tSTAR\ttranscript\t31\t60\t.\t+\t.\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_5\"; gene_name \"test_gene_2\";\n",
      "test_seq\tSTAR\texon\t31\t40\t.\t+\t.\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_5\"; exon_number \"1\"; gene_name \"test_gene_2\";\n",
      "test_seq\tSTAR\texon\t51\t60\t.\t+\t.\tgene_id \"TEST_GENE_2\"; transcript_id \"TRANSCRIPT_5\"; exon_number \"2\"; gene_name \"test_gene_2\";\n"
     ]
    }
   ],
   "source": [
    "cat test_gtf_file.gtf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fec83-ea41-4948-8cc3-bf3de2fc6bdb",
   "metadata": {},
   "source": [
    "With the GTF in hand, now the `create_annotator.py` script can be run to make the annotator pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac082e62-c4c8-4435-b861-c6f243797122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped exon boundary annotator to /home/jovyan/testsq_exon_bounds.pkl\n",
      "dumped splice junctions annotator to /home/jovyan/testsq_splices.pkl\n",
      "test_seq\n",
      "dumped annotator to /home/jovyan/testsq.pkl\n"
     ]
    }
   ],
   "source": [
    "%run scripts/create_annotator.py -g test_gtf_file.gtf -a testsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f71d62-b03b-4724-bfc8-a49b372e83a8",
   "metadata": {},
   "source": [
    "### Preparation STEP #3: set the settings in SICILIAN to reflect all this set up.\n",
    "\n",
    "The sections in `SICILIAN.py` where it says, 'Input arguments that should be set by the user' and 'Toggles for deciding which steps in SICILIAN should be run', lines 145 through 175, need to be edited to match the test data and settings desired. For example, I had thought I didn;t don't need to run STAR aligner again because ran it to set this up and so I planned to skip over by setting `run_map` to `False`. However, I found I needed it to make bam files in location that `SICILIAN.py` expects them and so I ended up not disabling it as I had intended.\n",
    "\n",
    "The string `c` will have the settings I want, and I made a small function to parse it and set them in the current `SICILIAN.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731ded63-ec26-4267-82f1-3f0fcd6b5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c='''###########################################################################################\n",
    "################## Input arguments that should be set by the user  ########################\n",
    "###########################################################################################\n",
    "  data_path = \"./\"\n",
    "  out_dir = \"sicilian_output\"\n",
    "  run_name = \"test\"\n",
    "  r_ends = [\".fq\",\".fq\"] #seems you need at least two or get `IndexError: list index out of range` for `command += \"--readFilesIn {}{}{} \".format(data_path, name, r_ends[i])`, but repeating bypasses issue\n",
    "  names = [\"test\"]\n",
    "  star_path = \"/srv/conda/envs/notebook/bin/STAR\"\n",
    "  star_ref_path = \"./test_index\"\n",
    "  gtf_file = \"./test_gtf_file.gtf\"\n",
    "  annotator_file = \"./testsq.pkl\"\n",
    "  exon_pickle_file = \"./testsq_exon_bounds.pkl\"\n",
    "  splice_pickle_file = \"./testsq_splices.pkl\"\n",
    "  domain_file = \"\"\n",
    "  single = True\n",
    "  tenX = False\n",
    "  stranded_library = False\n",
    "  bc_pattern = \"C\"*16 + \"N\"*12\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "## Toggles for deciding which steps in SICILIAN should be run #####\n",
    "  run_whitelist = False\n",
    "  run_extract = False\n",
    "  run_map = True\n",
    "  run_class = True\n",
    "  run_GLM = True\n",
    "###################################################################\n",
    "'''\n",
    "\n",
    "\n",
    "###--------------------------------------------------------------------------------------------------------###\n",
    "#### BELOW IS CODE TO MAKE THE SWAP. DON'T TOUCH BELOW HERE. ONLY MAKE EDITS TO ABOVE.########################\n",
    "import re\n",
    "# Define the pattern to match lines between markers\n",
    "start_point_string = \"Input arguments that should be set by the user\"\n",
    "end_point_string = \"run_GLM = True\"\n",
    "# Specify the file path\n",
    "script_file = \"./SICILIAN.py\"\n",
    "# Read the script file\n",
    "with open(script_file, \"r\") as file:\n",
    "    script_text = file.read()\n",
    "def swap_code_lines_in_SICILIAN_with_string_c(c):\n",
    "    pattern = r\"{}(.*?){}\".format(re.escape(start_point_string), re.escape(end_point_string))\n",
    "    # this next `match` is to set up to remove the evertyhing in front of and including `start_point_string` and `end_point_string` and everything after it from string `c`, so no duplicated borders points\n",
    "    match = re.search(fr\"{re.escape(start_point_string)}(.*?){re.escape(end_point_string)}\", c, re.DOTALL)\n",
    "    if match:\n",
    "        c_from_start_to_end = match.group(1)\n",
    "    else:\n",
    "        c_from_start_to_end = \"\"\n",
    "    replace = f\"{start_point_string}{c_from_start_to_end}{end_point_string}\"\n",
    "    # Replace the matched lines with the new code\n",
    "    new_script_text = re.sub(pattern, replace, script_text, flags=re.DOTALL)\n",
    "    # Write the modified script back to the file\n",
    "    with open(script_file, \"w\") as file:\n",
    "        file.write(new_script_text)\n",
    "swap_code_lines_in_SICILIAN_with_string_c(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d7bbb-63ef-4c22-8ac9-50b2b0ca1ba6",
   "metadata": {},
   "source": [
    "### Preparation STEP #4: Adjust to run in MyBinder session without Slurm.\n",
    "\n",
    "`SICILIAN.py` comes set up to use `sbatch` to run the jobs. `sbatch` submits a batch script to Slurm, which we don't have in this case.  \n",
    "Luckily, [here](https://github.com/salzman-lab/SICILIAN/issues/8#issuecomment-1059198873) and [here](https://github.com/salzman-lab/SICILIAN/issues/8#issuecomment-1709657122) already worked out how to convert it to use different way to run the associated jobs by replacing `sbatch`.  \n",
    "Running this next cell will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faea8a3d-61a8-41c0-a77c-2b02695b4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "script_file = \"./SICILIAN.py\"\n",
    "# Read the script file\n",
    "with open(script_file, \"r\") as file:\n",
    "    script_text = file.read()\n",
    "original_code_to_replace = '\"sbatch {}\"'\n",
    "replacement = '\"bash {}\"'\n",
    "new_script_text = script_text.replace(original_code_to_replace, replacement)\n",
    "# Write the modified script back to the file\n",
    "with open(script_file, \"w\") as file:\n",
    "    file.write(new_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7adba2-3983-4999-9c5f-143a296b48e5",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Run SICILIAN\n",
    "\n",
    "Run the main SICILIAN script by running the following cell. (If you were on the command line the following command would be more something along the lines of `python SICILIAN.py`. Here I'm using Jupyter's convenient magic that is more feature-rich.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9745a6ec-07fd-4593-9691-0dff2cc4011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 25 08:43:01 PM UTC 2024\n",
      "2.7.11b\n",
      "\t/srv/conda/envs/notebook/bin/STAR-avx2 --runThreadN 4 --genomeDir ./test_index --readFilesIn ./test.fq --twopassMode Basic --alignIntronMax 1000000 --outFileNamePrefix sicilian_output/test/test/2 --outSAMtype BAM Unsorted --outSAMattributes All --chimOutType WithinBAM SoftClip Junctions --chimJunctionOverhangMin 10 --chimSegmentReadGapMax 0 --chimOutJunctionFormat 1 --chimSegmentMin 12 --chimScoreJunctionNonGTAG -4 --chimNonchimScoreDropMin 10 --quantMode GeneCounts --sjdbGTFfile ./test_gtf_file.gtf --outReadsUnmapped Fastx\n",
      "\tSTAR version: 2.7.11b   compiled: 2024-07-03T14:39:20+0000 :/opt/conda/conda-bld/star_1720017372352/work/source\n",
      "Jul 25 20:43:01 ..... started STAR run\n",
      "Jul 25 20:43:01 ..... loading genome\n",
      "Jul 25 20:43:01 ..... processing annotations GTF\n",
      "Jul 25 20:43:01 ..... inserting junctions into the genome indices\n",
      "Jul 25 20:43:01 ..... started 1st pass mapping\n",
      "Jul 25 20:43:01 ..... finished 1st pass mapping\n",
      "Jul 25 20:43:01 ..... inserting junctions into the genome indices\n",
      "Jul 25 20:43:01 ..... started mapping\n",
      "Jul 25 20:43:02 ..... finished mapping\n",
      "Jul 25 20:43:02 ..... finished successfully\n",
      "Thu Jul 25 08:43:02 PM UTC 2024 (run_map.sh)\n",
      "Thu Jul 25 08:43:02 PM UTC 2024\n",
      "bam_files ['sicilian_output/test/test/2Aligned.out.bam']\n",
      "bam_files ['sicilian_output/test/test/2Aligned.out.bam']\n",
      "started modify 0.006744384765625\n",
      "ended modify 0.18961524963378906\n",
      "total time 0:00:00.240842\n",
      "Thu Jul 25 08:43:04 PM UTC 2024 (run_class_input.sh)\n",
      "Thu Jul 25 08:43:04 PM UTC 2024\n",
      "Loading required package: data.table\n",
      "Loading required package: glmnet\n",
      "Loading required package: Matrix\n",
      "Loaded glmnet 4.1-8\n",
      "Loading required package: tictoc\n",
      "\n",
      "Attaching package: ‘tictoc’\n",
      "\n",
      "The following object is masked from ‘package:data.table’:\n",
      "\n",
      "    shift\n",
      "\n",
      "Loading required package: dplyr\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:data.table’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Loading required package: stringr\n",
      "Loading required package: cutpointr\n",
      "Loading required package: GenomicAlignments\n",
      "Loading required package: BiocGenerics\n",
      "\n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    combine, intersect, setdiff, union\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n",
      "    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n",
      "    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n",
      "    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n",
      "    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n",
      "    table, tapply, union, unique, unsplit, which.max, which.min\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "Loading required package: stats4\n",
      "\n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    first, rename\n",
      "\n",
      "The following objects are masked from ‘package:Matrix’:\n",
      "\n",
      "    expand, unname\n",
      "\n",
      "The following objects are masked from ‘package:data.table’:\n",
      "\n",
      "    first, second\n",
      "\n",
      "The following object is masked from ‘package:utils’:\n",
      "\n",
      "    findMatches\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    expand.grid, I, unname\n",
      "\n",
      "Loading required package: IRanges\n",
      "\n",
      "Attaching package: ‘IRanges’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    collapse, desc, slice\n",
      "\n",
      "The following object is masked from ‘package:tictoc’:\n",
      "\n",
      "    shift\n",
      "\n",
      "The following object is masked from ‘package:data.table’:\n",
      "\n",
      "    shift\n",
      "\n",
      "Loading required package: GenomeInfoDb\n",
      "Loading required package: GenomicRanges\n",
      "Loading required package: SummarizedExperiment\n",
      "Loading required package: MatrixGenerics\n",
      "Loading required package: matrixStats\n",
      "\n",
      "Attaching package: ‘matrixStats’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    count\n",
      "\n",
      "\n",
      "Attaching package: ‘MatrixGenerics’\n",
      "\n",
      "The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n",
      "    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n",
      "    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n",
      "    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n",
      "    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n",
      "    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n",
      "    colWeightedMeans, colWeightedMedians, colWeightedSds,\n",
      "    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n",
      "    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n",
      "    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n",
      "    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n",
      "    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n",
      "    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n",
      "    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n",
      "    rowWeightedSds, rowWeightedVars\n",
      "\n",
      "Loading required package: Biobase\n",
      "Welcome to Bioconductor\n",
      "\n",
      "    Vignettes contain introductory material; view with\n",
      "    'browseVignettes()'. To cite Bioconductor, see\n",
      "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
      "\n",
      "\n",
      "Attaching package: ‘Biobase’\n",
      "\n",
      "The following object is masked from ‘package:MatrixGenerics’:\n",
      "\n",
      "    rowMedians\n",
      "\n",
      "The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    anyMissing, rowMedians\n",
      "\n",
      "Loading required package: Biostrings\n",
      "Loading required package: XVector\n",
      "\n",
      "Attaching package: ‘Biostrings’\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    strsplit\n",
      "\n",
      "Loading required package: Rsamtools\n",
      "\n",
      "Attaching package: ‘GenomicAlignments’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    last\n",
      "\n",
      "The following object is masked from ‘package:data.table’:\n",
      "\n",
      "    last\n",
      "\n",
      "reading inputs:: 0.001 sec elapsed\n",
      "unstranded data modification: 0.193 sec elapsed\n",
      "Warning messages:\n",
      "1: In max(juncPosR1A, juncPosR1B) :\n",
      "  no non-missing arguments to max; returning -Inf\n",
      "2: In min(juncPosR1A, juncPosR1B) :\n",
      "  no non-missing arguments to min; returning Inf\n",
      "Warning messages:\n",
      "1: In `[.data.table`(class_input, , `:=`(c(\"junc_cdf_glm\", \"junc_cdf_glm_corrected\",  :\n",
      "  Column 'junc_cdf_glm' does not exist to remove\n",
      "2: In `[.data.table`(class_input, , `:=`(c(\"junc_cdf_glm\", \"junc_cdf_glm_corrected\",  :\n",
      "  Column 'junc_cdf_glm_corrected' does not exist to remove\n",
      "3: In `[.data.table`(class_input, , `:=`(c(\"junc_cdf_glm\", \"junc_cdf_glm_corrected\",  :\n",
      "  Column 'junc_cdf_glmnet' does not exist to remove\n",
      "4: In `[.data.table`(class_input, , `:=`(c(\"junc_cdf_glm\", \"junc_cdf_glm_corrected\",  :\n",
      "  Column 'junc_cdf_glmnet_constrained' does not exist to remove\n",
      "5: In `[.data.table`(class_input, , `:=`(c(\"junc_cdf_glm\", \"junc_cdf_glm_corrected\",  :\n",
      "  Column 'junc_cdf_glmnet_corrected' does not exist to remove\n",
      "6: In `[.data.table`(class_input, , `:=`(c(\"junc_cdf_glm\", \"junc_cdf_glm_corrected\",  :\n",
      "  Column 'junc_cdf_glmnet_corrected_constrained' does not exist to remove\n",
      "Warning messages:\n",
      "1: In `[.data.table`(class_input, , `:=`(c(\"p_predicted_glm\", \"p_predicted_corrected\",  :\n",
      "  Column 'p_predicted_glm' does not exist to remove\n",
      "2: In `[.data.table`(class_input, , `:=`(c(\"p_predicted_glm\", \"p_predicted_corrected\",  :\n",
      "  Column 'p_predicted_corrected' does not exist to remove\n",
      "3: In `[.data.table`(class_input, , `:=`(c(\"p_predicted_glm\", \"p_predicted_corrected\",  :\n",
      "  Column 'p_predicted_glmnet' does not exist to remove\n",
      "4: In `[.data.table`(class_input, , `:=`(c(\"p_predicted_glm\", \"p_predicted_corrected\",  :\n",
      "  Column 'p_predicted_glmnet_constrained' does not exist to remove\n",
      "5: In `[.data.table`(class_input, , `:=`(c(\"p_predicted_glm\", \"p_predicted_corrected\",  :\n",
      "  Column 'p_predicted_glmnet_corrected' does not exist to remove\n",
      "6: In `[.data.table`(class_input, , `:=`(c(\"p_predicted_glm\", \"p_predicted_corrected\",  :\n",
      "  Column 'p_predicted_glmnet_corrected_constrained' does not exist to remove\n",
      "Error in base::strsplit(x, ...) : non-character argument\n",
      "Calls: [ ... [.data.table -> strsplit -> strsplit -> <Anonymous>\n",
      "Execution halted\n",
      "Thu Jul 25 08:43:24 PM UTC 2024 (run_GLM.sh)\n"
     ]
    }
   ],
   "source": [
    "%run SICILIAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be2458-ea38-4fc3-a422-56026740dc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
